import scipy as sp
import numpy as np
import pandas as pd
import seaborn as sns
from scipy import stats
import statsmodels.api as sm
from pathlib import Path
import os
from rqdatac import *
from rqfactor import *
from rqfactor import Factor
from rqfactor.extension import *

init("13522652015", "123456")
import rqdatac

from tqdm import *

import matplotlib.pyplot as plt


# ==================== æ•°æ®è·¯å¾„ç®¡ç† ====================

def get_data_path(data_type, filename=None, auto_create=True, **kwargs):
    """
    ç»Ÿä¸€çš„æ•°æ®è·¯å¾„ç®¡ç†å‡½æ•°
    
    å‚æ•°:
        data_type: æ•°æ®ç±»å‹
            - 'combo_mask': ç»„åˆæ©ç æ•°æ® -> data/cache/combo_masks/
            - 'return_1d': æ—¥æ”¶ç›Šç‡æ•°æ® -> data/cache/returns/
            - 'industry_market': è¡Œä¸šå¸‚å€¼æ•°æ® -> data/cache/industry/
            - 'open_price': å¼€ç›˜ä»·æ•°æ® -> data/cache/market_data/
            - 'factor_raw': åŸå§‹å› å­æ•°æ® -> data/factor_lib/raw/
            - 'factor_processed': å¤„ç†åå› å­æ•°æ® -> data/factor_lib/processed/
        filename: æ–‡ä»¶åï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™æ ¹æ®kwargsè‡ªåŠ¨ç”Ÿæˆï¼‰
        auto_create: æ˜¯å¦è‡ªåŠ¨åˆ›å»ºç›®å½•
        **kwargs: ç”¨äºç”Ÿæˆæ–‡ä»¶åçš„å‚æ•°
    
    è¿”å›:
        å®Œæ•´çš„æ–‡ä»¶è·¯å¾„
    """
    
    # è·¯å¾„æ˜ å°„
    path_mapping = {
        'combo_mask': 'data/cache/combo_masks',
        'return_1d': 'data/cache/returns',
        'industry_market': 'data/cache/industry', 
        'open_price': 'data/cache/market_data',
        'factor_raw': 'data/factor_lib/raw',
        'factor_processed': 'data/factor_lib/processed'
    }
    
    # æ–‡ä»¶åæ¨¡æ¿
    filename_templates = {
        'combo_mask': "combo_mask_{index_item}_{start_date}_{end_date}.pkl",
        'return_1d': "return_1d_{index_item}_{start}_{end}.pkl",
        'industry_market': "df_industry_market_{industry_type}_{index_item}_{start}_{end}.pkl",
        'open_price': "open_{index_item}_{start}_{end}.pkl"
    }
    
    if data_type not in path_mapping:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {data_type}ã€‚æ”¯æŒçš„ç±»å‹: {list(path_mapping.keys())}")
    
    # ç”Ÿæˆæ–‡ä»¶å
    if filename is None:
        if data_type in filename_templates:
            filename = filename_templates[data_type].format(**kwargs)
        else:
            raise ValueError(f"æ•°æ®ç±»å‹ {data_type} éœ€è¦æä¾› filename å‚æ•°")
    
    # æ™ºèƒ½è·¯å¾„è§£æï¼šå¦‚æœå½“å‰åœ¨scriptsç›®å½•ï¼Œåˆ™è¿”å›ä¸Šçº§ç›®å½•
    current_dir = os.getcwd()
    if current_dir.endswith('/scripts') or current_dir.endswith('\\scripts'):
        # ä» scripts ç›®å½•è°ƒç”¨ï¼Œéœ€è¦è¿”å›ä¸Šçº§ç›®å½•
        base_path = os.path.join('..', path_mapping[data_type])
    else:
        # ä»å…¶ä»–ç›®å½•è°ƒç”¨ï¼Œä½¿ç”¨ç›¸å¯¹è·¯å¾„
        base_path = path_mapping[data_type]
    
    # æ„å»ºå®Œæ•´è·¯å¾„
    full_path = os.path.join(base_path, filename)
    
    # è‡ªåŠ¨åˆ›å»ºç›®å½•
    if auto_create:
        os.makedirs(os.path.dirname(full_path), exist_ok=True)
    
    return full_path


def migrate_file_to_new_structure(old_path, data_type, **kwargs):
    """
    å°†æ–‡ä»¶ä»æ—§è·¯å¾„è¿ç§»åˆ°æ–°è·¯å¾„ç»“æ„
    
    å‚æ•°:
        old_path: æ—§æ–‡ä»¶è·¯å¾„
        data_type: æ–°çš„æ•°æ®ç±»å‹
        **kwargs: ç”¨äºç”Ÿæˆæ–°æ–‡ä»¶åçš„å‚æ•°
    """
    if os.path.exists(old_path):
        new_path = get_data_path(data_type, **kwargs)
        if not os.path.exists(new_path):
            import shutil
            shutil.move(old_path, new_path)
            print(f"æ–‡ä»¶å·²è¿ç§»: {old_path} -> {new_path}")
        return new_path
    return None


def load_or_create_data(data_type, create_func, **kwargs):
    """
    ç®€åŒ–çš„æ•°æ®åŠ è½½å‡½æ•°ï¼šå°è¯•åŠ è½½ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
    
    å‚æ•°:
        data_type: æ•°æ®ç±»å‹
        create_func: åˆ›å»ºæ•°æ®çš„å‡½æ•°
        **kwargs: ç”¨äºç”Ÿæˆæ–‡ä»¶åçš„å‚æ•°
    
    è¿”å›:
        åŠ è½½æˆ–åˆ›å»ºçš„æ•°æ®
    """
    data_path = get_data_path(data_type, **kwargs)
    
    try:
        return pd.read_pickle(data_path)
    except:
        data = create_func()
        data.to_pickle(data_path)
        return data

plt.rcParams["font.sans-serif"] = [
    "Arial Unicode MS",
    "PingFang SC",
    "Hiragino Sans GB",
    "STHeiti",
    "DejaVu Sans",
]
plt.rcParams["axes.unicode_minus"] = False

import warnings

warnings.filterwarnings("ignore")


# çƒ­åŠ›å›¾
def hot_corr(name, ic_df):
    """
    :param name: å› å­åç§° -> list
    :param ic_df: icåºåˆ—è¡¨ -> dataframe
    :return fig: çƒ­åŠ›å›¾ -> plt
    """
    ax = plt.subplots(figsize=(len(name), len(name)))  # è°ƒæ•´ç”»å¸ƒå¤§å°
    ax = sns.heatmap(
        ic_df[name].corr(), vmin=0.4, square=True, annot=True, cmap="Greens"
    )  # annot=True è¡¨ç¤ºæ˜¾ç¤ºç³»æ•°
    plt.title("Factors_IC_CORRELATION")
    # è®¾ç½®åˆ»åº¦å­—ä½“å¤§å°
    plt.xticks(fontsize=10)
    plt.yticks(fontsize=10)


# åŠ¨æ€åˆ¸æ± 
def INDEX_FIX(start_date, end_date, index_item):
    """
    :param start_date: å¼€å§‹æ—¥ -> str
    :param end_date: ç»“æŸæ—¥ -> str
    :param index_item: æŒ‡æ•°ä»£ç  -> str
    :return index_fix: åŠ¨æ€å› å­å€¼ -> unstack
    """

    index_fix = pd.DataFrame(
        {
            k: dict.fromkeys(v, True)
            for k, v in index_components(
                index_item, start_date=start_date, end_date=end_date
            ).items()
        }
    ).T

    index_fix.fillna(False, inplace=True)

    return index_fix


# æ–°è‚¡è¿‡æ»¤
def get_new_stock_filter(stock_list, datetime_period, newly_listed_threshold=252):
    """
    :param stock_list: è‚¡ç¥¨é˜Ÿåˆ— -> list
    :param datetime_period: ç ”ç©¶å‘¨æœŸ -> list
    :param newly_listed_threshold: æ–°è‚¡æ—¥æœŸé˜ˆå€¼ -> int
    :return newly_listed_window: æ–°è‚¡è¿‡æ»¤åˆ¸æ±  -> unstack
    """

    datetime_period_tmp = datetime_period.copy()
    # å¤šæ·»åŠ ä¸€å¤©
    datetime_period_tmp += [
        pd.to_datetime(get_next_trading_date(datetime_period[-1], 1))
    ]
    # è·å–ä¸Šå¸‚æ—¥æœŸ
    listed_datetime_period = [instruments(stock).listed_date for stock in stock_list]
    # è·å–ä¸Šå¸‚åçš„ç¬¬252ä¸ªäº¤æ˜“æ—¥ï¼ˆæ–°è‚¡å’Œè€è‚¡çš„åˆ†ç•Œç‚¹ï¼‰
    newly_listed_window = pd.Series(
        index=stock_list,
        data=[
            pd.to_datetime(get_next_trading_date(listed_date, n=newly_listed_threshold))
            for listed_date in listed_datetime_period
        ],
    )
    # é˜²æ­¢åˆ†å‰²æ—¥åœ¨ç ”ç©¶æ—¥ä¹‹åï¼Œåç»­å¡«å……ä¸å­˜åœ¨
    for k, v in enumerate(newly_listed_window):
        if v > datetime_period_tmp[-1]:
            newly_listed_window.iloc[k] = datetime_period_tmp[-1]

    # æ ‡ç­¾æ–°è‚¡ï¼Œæ„å»ºè¿‡æ»¤è¡¨æ ¼
    newly_listed_window.index.names = ["order_book_id"]
    newly_listed_window = newly_listed_window.to_frame("date")
    newly_listed_window["signal"] = True
    newly_listed_window = (
        newly_listed_window.reset_index()
        .set_index(["date", "order_book_id"])
        .signal.unstack("order_book_id")
        .reindex(index=datetime_period_tmp)
    )
    newly_listed_window = newly_listed_window.shift(-1).bfill().fillna(False).iloc[:-1]

    return newly_listed_window


# stè¿‡æ»¤ï¼ˆé£é™©è­¦ç¤ºæ ‡çš„é»˜è®¤ä¸è¿›è¡Œç ”ç©¶ï¼‰
def get_st_filter(stock_list, datetime_period):
    """
    :param stock_list: è‚¡ç¥¨æ±  -> list
    :param datetime_period: ç ”ç©¶å‘¨æœŸ -> list
    :return st_filter: stè¿‡æ»¤åˆ¸æ±  -> unstack
    """

    # å½“stæ—¶è¿”å›1ï¼Œéstæ—¶è¿”å›0
    st_filter = is_st_stock(
        stock_list, datetime_period[0], datetime_period[-1]
    ).reindex(columns=stock_list, index=datetime_period)
    st_filter = st_filter.shift(-1).ffill()

    return st_filter


# åœç‰Œè¿‡æ»¤ ï¼ˆæ— æ³•äº¤æ˜“ï¼‰
def get_suspended_filter(stock_list, date_list):
    """
    :param stock_list: è‚¡ç¥¨æ±  -> list
    :param date_list: ç ”ç©¶å‘¨æœŸ -> list
    :return suspended_filter: åœç‰Œè¿‡æ»¤åˆ¸æ±  -> unstack
    """

    # å½“åœç‰Œæ—¶è¿”å›1ï¼Œéåœç‰Œæ—¶è¿”å›0
    suspended_filter = is_suspended(stock_list, date_list[0], date_list[-1]).reindex(
        columns=stock_list, index=date_list
    )
    suspended_filter = suspended_filter.shift(-1).ffill()

    return suspended_filter


# æ¶¨åœè¿‡æ»¤ ï¼ˆå¼€ç›˜æ— æ³•ä¹°å…¥ï¼‰
def get_limit_up_filter(stock_list, date_list):
    """
    :param stock_list: è‚¡ç¥¨æ±  -> list
    :param date_list: ç ”ç©¶å‘¨æœŸ -> list
    :return limit_up_filter: æ¶¨åœè¿‡æ»¤åˆ¸æ±  -> unstack
    """
    # æ¶¨åœæ—¶è¿”å›ä¸º1,éæ¶¨åœè¿”å›ä¸º0
    price = get_price(
        stock_list,
        date_list[0],
        date_list[-1],
        adjust_type="none",
        fields=["open", "limit_up"],
    )
    df = (
        (price["open"] == price["limit_up"])
        .unstack("order_book_id")
        .shift(-1)
        .fillna(False)
    )

    return df


# ç¦»ç¾¤å€¼å¤„ç†
def mad_vectorized(df, n=3 * 1.4826):

    # è®¡ç®—æ¯è¡Œçš„ä¸­ä½æ•°
    median = df.median(axis=1)
    # è®¡ç®—æ¯è¡Œçš„MAD
    mad_values = (df.sub(median, axis=0).abs()).median(axis=1)
    # è®¡ç®—ä¸Šä¸‹ç•Œ
    lower_bound = median - n * mad_values
    upper_bound = median + n * mad_values

    return df.clip(lower_bound, upper_bound, axis=0)


# æ ‡å‡†åŒ–å¤„ç†
def standardize(df):
    df_standardize = df.sub(df.mean(axis=1), axis=0).div(df.std(axis=1), axis=0)
    return df_standardize


# è·å–è¡Œä¸šæš´éœ²åº¦çŸ©é˜µ
def get_industry_exposure(order_book_ids, datetime_period, industry_type="zx"):
    """
    :param order_book_ids: è‚¡ç¥¨æ±  -> list
    :param datetime_period: ç ”ç©¶æ—¥ -> list
    :param industry_type: è¡Œä¸šåˆ†ç±»æ ‡å‡† äºŒé€‰ä¸€ ä¸­ä¿¡/ç”³ä¸‡ zx/sw -> str
    :return result: è™šæ‹Ÿå˜é‡ -> df_unstack
    """

    # è·å–è¡Œä¸šç‰¹å¾æ•°æ®
    if industry_type == "zx":
        industry_map_dict = rqdatac.client.get_client().execute(
            "__internal__zx2019_industry"
        )
        # æ„å»ºä¸ªè‚¡/è¡Œä¸šmap
        df = pd.DataFrame(
            industry_map_dict,
            columns=["first_industry_name", "order_book_id", "start_date"],
        )
        df.sort_values(["order_book_id", "start_date"], ascending=True, inplace=True)
        df = df.pivot(
            index="start_date", columns="order_book_id", values="first_industry_name"
        ).ffill()
    else:
        industry_map_dict = rqdatac.client.get_client().execute(
            "__internal__shenwan_industry"
        )
        df = pd.DataFrame(
            industry_map_dict,
            columns=["index_name", "order_book_id", "version", "start_date"],
        )
        df = df[df.version == 2]
        df = df.drop_duplicates()
        df = (
            df.set_index(["start_date", "order_book_id"])
            .index_name.unstack("order_book_id")
            .ffill()
        )

    # åŒ¹é…äº¤æ˜“æ—¥
    datetime_period_base = pd.to_datetime(
        get_trading_dates(get_previous_trading_date(df.index[0], 2), df.index[-1])
    )
    df.index = datetime_period_base.take(
        datetime_period_base.searchsorted(df.index, side="right") - 1
    )
    # åˆ‡ç‰‡æ‰€éœ€æ—¥æœŸ
    df = df.reset_index().drop_duplicates(subset=["index"]).set_index("index")
    df = (
        df.reindex(index=datetime_period_base)
        .ffill()
        .reindex(index=datetime_period)
        .ffill()
    )
    inter_stock_list = list(set(df.columns) & set(order_book_ids))
    df = df[inter_stock_list].sort_index(axis=1)

    # ç”Ÿæˆè¡Œä¸šè™šæ‹Ÿå˜é‡
    return df.stack()


# è¡Œä¸šå¸‚å€¼ä¸­æ€§åŒ–
def neutralization_vectorized(
    factor,
    order_book_ids,
    index_item="",
    industry_type="zx",
):

    datetime_period = factor.index.tolist()
    start = datetime_period[0].strftime("%F")
    end = datetime_period[-1].strftime("%F")

    # å°è¯•ä»æ–°è·¯å¾„è¯»å–industry_marketæ•°æ®
    new_path = get_data_path('industry_market', 
                            industry_type=industry_type,
                            index_item=index_item, 
                            start=start, 
                            end=end)
    legacy_path = get_legacy_path(f"df_industry_market_{industry_type}_{index_item}_{start}_{end}.pkl")
    
    df_industry_market = None
    
    # ä¼˜å…ˆå°è¯•æ–°è·¯å¾„
    try:
        df_industry_market = pd.read_pickle(new_path)
        print(f"âœ… ä»æ–°è·¯å¾„åŠ è½½industry_market: {new_path}")
    except:
        # å°è¯•æ—§è·¯å¾„
        try:
            df_industry_market = pd.read_pickle(legacy_path)
            print(f"âš ï¸  ä»æ—§è·¯å¾„åŠ è½½industry_market: {legacy_path}")
            # è‡ªåŠ¨è¿ç§»åˆ°æ–°è·¯å¾„
            df_industry_market.to_pickle(new_path)
            print(f"âœ… å·²è¿ç§»industry_marketåˆ°æ–°è·¯å¾„: {new_path}")
        except:
            print(f"ğŸ“ è®¡ç®—æ–°çš„industry_market...")
            pass
    
    # å¦‚æœéƒ½æ²¡æœ‰æ‰¾åˆ°ï¼Œåˆ™é‡æ–°è®¡ç®—
    if df_industry_market is None:
        # è·å–å¸‚å€¼æš´éœ²åº¦
        market_cap = (
            execute_factor(LOG(Factor("market_cap_3")), order_book_ids, start, end)
            .stack()
            .to_frame("market_cap")
        )
        # è·å–è¡Œä¸šæš´éœ²åº¦
        industry_df = pd.get_dummies(
            get_industry_exposure(order_book_ids, datetime_period, industry_type)
        ).astype(int)
        # åˆå¹¶å¸‚å€¼è¡Œä¸šæš´éœ²åº¦
        industry_df["market_cap"] = market_cap
        df_industry_market = industry_df
        df_industry_market.index.names = ["datetime", "order_book_id"]
        df_industry_market.dropna(axis=0, inplace=True)
        
        # ä¿å­˜åˆ°æ–°è·¯å¾„
        df_industry_market.to_pickle(new_path)
        print(f"ğŸ’¾ industry_marketå·²ä¿å­˜åˆ°: {new_path}")

    df_industry_market["factor"] = factor.stack()
    df_industry_market.dropna(subset="factor", inplace=True)

    # å°†factoråˆ—ç§»åˆ°ç¬¬ä¸€åˆ—
    cols = df_industry_market.columns.tolist()
    cols.remove("factor")
    df_industry_market = df_industry_market[["factor"] + cols]

    # æˆªé¢å›å½’
    def neutralize_cross_section(group):

        try:
            y = group["factor"]
            x = group.iloc[:, 1:]
            model = sm.OLS(y, x, hasconst=False, missing="drop").fit()
            return pd.Series(model.resid)
        except:
            return pd.Series(dtype=float)

    factor_resid = df_industry_market.groupby(level=0).apply(neutralize_cross_section)
    factor_resid = factor_resid.reset_index(level=0, drop=True)
    factor_resid = factor_resid.unstack(level=0).T
    factor_resid.index = pd.to_datetime(factor_resid.index)

    return factor_resid


# å•å› å­æ£€éªŒ
def calc_ic(df, n, index_item, name="", Rank_IC=True):

    # åŸºç¡€æ•°æ®è·å–
    order_book_ids = df.columns.tolist()
    datetime_period = df.index
    start = datetime_period.min().strftime("%F")
    end = datetime_period.max().strftime("%F")

    # å°è¯•ä»æ–°è·¯å¾„è¯»å–open_priceæ•°æ®
    new_path = get_data_path('open_price', index_item=index_item, start=start, end=end)
    legacy_path = get_legacy_path(f"open_{index_item}_{start}_{end}.pkl")
    
    open = None
    
    # ä¼˜å…ˆå°è¯•æ–°è·¯å¾„
    try:
        open = pd.read_pickle(new_path)
        print(f"âœ… ä»æ–°è·¯å¾„åŠ è½½open_price: {new_path}")
    except:
        # å°è¯•æ—§è·¯å¾„
        try:
            open = pd.read_pickle(legacy_path)
            print(f"âš ï¸  ä»æ—§è·¯å¾„åŠ è½½open_price: {legacy_path}")
            # è‡ªåŠ¨è¿ç§»åˆ°æ–°è·¯å¾„
            open.to_pickle(new_path)
            print(f"âœ… å·²è¿ç§»open_priceåˆ°æ–°è·¯å¾„: {new_path}")
        except:
            print(f"ğŸ“ è®¡ç®—æ–°çš„open_price...")
            pass
    
    # å¦‚æœéƒ½æ²¡æœ‰æ‰¾åˆ°ï¼Œåˆ™é‡æ–°è®¡ç®—
    if open is None:
        # æ‹¿ä¸€ä¸ªå®Œæ•´çš„åˆ¸æ± è¡¨æ ¼ï¼Œé˜²æ­¢æœ‰äº›è‚¡ç¥¨åœ¨æŸäº›æ—¥æœŸæ²¡æœ‰æ•°æ®ï¼Œå¯¼è‡´ç¼“å­˜æ•°æ®ä¸å…¨ï¼Œå½±å“å…¶ä»–å› å­è®¡ç®—
        index_fix = INDEX_FIX(start, end, index_item)
        order_book_ids = index_fix.columns.tolist()
        datetime_period = index_fix.index.tolist()
        # è·å–å¼€ç›˜ä»·
        open = get_price(
            order_book_ids,
            start_date=start,
            end_date=end,
            frequency="1d",
            fields="open",
        ).open.unstack("order_book_id")
        
        # ä¿å­˜åˆ°æ–°è·¯å¾„
        open.to_pickle(new_path)
        print(f"ğŸ’¾ open_priceå·²ä¿å­˜åˆ°: {new_path}")

    # æœªæ¥ä¸€æ®µæ”¶ç›Šè‚¡ç¥¨çš„ç´¯è®¡æ”¶ç›Šç‡è®¡ç®—
    return_n = open.pct_change(n).shift(-n - 1)

    # è®¡ç®—IC
    if Rank_IC == True:
        result = df.corrwith(return_n, axis=1, method="spearman").dropna(how="all")
    else:
        result = df.corrwith(return_n, axis=1, method="pearson").dropna(how="all")

    # tæ£€éªŒ å•æ ·æœ¬
    t_stat, _ = stats.ttest_1samp(result, 0)

    # å› å­æŠ¥å‘Š
    report = {
        "name": name,
        "change_day": n,
        "IC mean": round(result.mean(), 4),
        # "IC std": round(result.std(), 4),
        "ICIR": round(result.mean() / result.std(), 4),
        "IC>0": round(len(result[result > 0].dropna()) / len(result), 4),
        "ABS_IC>2%": round(len(result[abs(result) > 0.02].dropna()) / len(result), 4),
        "t_stat": round(t_stat, 4),
    }

    print(report)

    report = pd.DataFrame([report])

    return result, report


# åˆ†ç»„å›æµ‹
def group_g(df, n, g, index_item, name="", rebalance=False):
    """
    :param df: å› å­å€¼ -> unstack
    :param n: è°ƒä»“æ—¥ -> int
    :param g: åˆ†ç»„æ•°é‡ -> int
    :param index_item: åˆ¸æ± å -> str
    :param name: å› å­å -> str
    :param rebalance: æ˜¯å¦rebalance -> bool
    :return group_return: å„åˆ†ç»„æ—¥æ”¶ç›Šç‡ -> dataframe
    :return turnover_ratio: å„åˆ†ç»„æ—¥è°ƒä»“æ—¥æ¢æ‰‹ç‡ -> dataframe
    """

    # ä¿¡å·å‘åç§»åŠ¨ä¸€å¤©
    df = df.shift(1).iloc[1:]

    # åŸºç¡€æ•°æ®è·å–
    order_book_ids = df.columns.tolist()
    datetime_period = df.index
    start = datetime_period.min().strftime("%F")
    end = datetime_period.max().strftime("%F")

    # æå–é¢„å­˜å‚¨æ•°æ®
    # å°è¯•ä»æ–°è·¯å¾„è¯»å–return_1dæ•°æ®
    new_path = get_data_path('return_1d', index_item=index_item, start=start, end=end)
    legacy_path = get_legacy_path(f"return_1d_{index_item}_{start}_{end}.pkl")
    
    return_1d = None
    
    # ä¼˜å…ˆå°è¯•æ–°è·¯å¾„
    try:
        return_1d = pd.read_pickle(new_path)
        print(f"âœ… ä»æ–°è·¯å¾„åŠ è½½return_1d: {new_path}")
    except:
        # å°è¯•æ—§è·¯å¾„
        try:
            return_1d = pd.read_pickle(legacy_path)
            print(f"âš ï¸  ä»æ—§è·¯å¾„åŠ è½½return_1d: {legacy_path}")
            # è‡ªåŠ¨è¿ç§»åˆ°æ–°è·¯å¾„
            return_1d.to_pickle(new_path)
            print(f"âœ… å·²è¿ç§»return_1dåˆ°æ–°è·¯å¾„: {new_path}")
        except:
            print(f"ğŸ“ è®¡ç®—æ–°çš„return_1d...")
            pass
    
    # å¦‚æœéƒ½æ²¡æœ‰æ‰¾åˆ°ï¼Œåˆ™é‡æ–°è®¡ç®—
    if return_1d is None:
        # æ‹¿ä¸€ä¸ªå®Œæ•´çš„åˆ¸æ± è¡¨æ ¼ï¼Œé˜²æ­¢æœ‰äº›è‚¡ç¥¨åœ¨æŸäº›æ—¥æœŸæ²¡æœ‰æ•°æ®ï¼Œå¯¼è‡´ç¼“å­˜æ•°æ®ä¸å…¨ï¼Œå½±å“å…¶ä»–å› å­è®¡ç®—
        index_fix = INDEX_FIX(start, end, index_item)
        order_book_ids = index_fix.columns.tolist()
        # æœªæ¥ä¸€å¤©æ”¶ç›Šç‡
        open = get_price(
            order_book_ids,
            start,
            get_next_trading_date(end, 1),
            "1d",
            "open",
            "pre",
            False,
            True,
        ).open.unstack("order_book_id")
        return_1d = open.pct_change().shift(-1).dropna(axis=0, how="all").stack()
        
        # ä¿å­˜åˆ°æ–°è·¯å¾„
        return_1d.to_pickle(new_path)
        print(f"ğŸ’¾ return_1då·²ä¿å­˜åˆ°: {new_path}")

    # æ•°æ®å’Œæ”¶ç›Šåˆå¹¶
    group = df.stack().to_frame("factor")
    group["current_renturn"] = return_1d
    group = group.dropna()
    group.reset_index(inplace=True)
    group.columns = ["date", "stock", "factor", "current_renturn"]

    # æ¢æ‰‹ç‡ å’Œ åˆ†ç»„æ”¶ç›Šç‡è¡¨æ ¼
    turnover_ratio = pd.DataFrame()
    group_return = pd.DataFrame()

    datetime_period = pd.to_datetime(group.date.unique())

    # æŒ‰æ­¥é•¿å‘¨æœŸè°ƒä»“
    for i in range(0, len(datetime_period) - 1, n):  # -1 é˜²æ­¢åˆšå¥½åˆ‡åˆ°æœ€åä¸€å¤©æ²¡æ³•è®¡ç®—
        # æˆªé¢åˆ†ç»„
        single = group[group.date == datetime_period[i]].sort_values(by="factor")
        # æ ¹æ®å€¼çš„å¤§å°è¿›è¡Œåˆ‡åˆ†
        single.loc[:, "group"] = pd.qcut(
            single.factor, g, list(range(1, g + 1))
        ).to_list()
        group_dict = {}
        # åˆ†ç»„å†…çš„è‚¡ç¥¨
        for j in range(1, g + 1):
            group_dict[j] = single[single.group == j].stock.tolist()

        # è®¡ç®—æ¢æ‰‹ç‡
        turnover_ratio_temp = []
        if i == 0:
            # é¦–æœŸåˆ†ç»„æˆåˆ†è‚¡ å­˜å…¥å†å²
            temp_group_dict = group_dict
        else:
            # åˆ†ç»„è®¡ç®—æ¢æ‰‹ç‡
            for j in range(1, g + 1):
                turnover_ratio_temp.append(
                    len(list(set(temp_group_dict[j]).difference(set(group_dict[j]))))
                    / len(set(temp_group_dict[j]))
                )
            # å­˜å‚¨åˆ†ç»„æ¢æ‰‹ç‡
            turnover_ratio = pd.concat(
                [
                    turnover_ratio,
                    pd.DataFrame(
                        turnover_ratio_temp,
                        index=["G{}".format(j) for j in list(range(1, g + 1))],
                        columns=[datetime_period[i]],
                    ).T,
                ],
                axis=0,
            )
            # å­˜å…¥å†å²
            temp_group_dict = group_dict

        # è·å–å‘¨æœŸ
        # ä¸å¤Ÿä¸€ä¸ªè°ƒä»“å‘¨æœŸï¼Œå‰©ä¸‹çš„éƒ½æ˜¯æœ€åä¸€ä¸ªå‘¨æœŸ
        if i < len(datetime_period) - n:
            period = group[group.date.isin(datetime_period[i : i + n])]
        else:
            # å®Œæ•´å‘¨æœŸ
            period = group[group.date.isin(datetime_period[i:])]

        if i == 2540:
            breakpoint()

        # è®¡ç®—å„åˆ†ç»„æ”¶ç›Šç‡ï¼ˆæœŸé—´ä¸rebalanceæƒé‡ï¼‰
        group_return_temp = []
        for j in range(1, g + 1):
            if rebalance:
                # æ¨ªæˆªé¢æ±‡æ€»
                group_ret = period[period.stock.isin(group_dict[j])]
                group_ret = group_ret.set_index(
                    ["date", "stock"]
                ).current_renturn.unstack("stock")
                group_ret_combine_ret = group_ret.mean(axis=1)

            else:
                # ç»„å†…å„æ ‡çš„æ•°æ®
                group_ret = period[period.stock.isin(group_dict[j])]
                group_ret = group_ret.set_index(
                    ["date", "stock"]
                ).current_renturn.unstack("stock")
                # æ ‡çš„ç´¯è®¡æ”¶ç›Š
                group_ret_combine_cumnet = (1 + group_ret).cumprod().mean(axis=1)
                # ç»„åˆçš„é€æœŸæ”¶ç›Š
                group_ret_combine_ret = group_ret_combine_cumnet.pct_change()
                # ç¬¬ä¸€æœŸå¡«è¡¥
                group_ret_combine_ret.iloc[0] = group_ret.iloc[0].mean()

            # åˆå¹¶å„åˆ†ç»„
            group_return_temp.append(group_ret_combine_ret)

        # æ¯ä¸ªæ­¥é•¿æœŸé—´çš„æ”¶ç›Šåˆå¹¶
        group_return = pd.concat(
            [
                group_return,
                pd.DataFrame(
                    group_return_temp,
                    index=["G{}".format(j) for j in list(range(1, g + 1))],
                ).T,
            ],
            axis=0,
        )
        # è¿›åº¦
        print("\r å½“å‰ï¼š{} / æ€»é‡ï¼š{}".format(i, len(datetime_period)), end="")

    # åŸºå‡†ï¼Œå„ç»„çš„å¹³å‡æ”¶ç›Š
    group_return["Benchmark"] = group_return.mean(axis=1)
    group_return = (group_return + 1).cumprod()
    # å¹´åŒ–æ”¶ç›Šè®¡ç®—
    group_annual_ret = group_return.iloc[-1] ** (252 / len(group_return)) - 1
    group_annual_ret -= group_annual_ret.Benchmark
    group_annual_ret = group_annual_ret.drop("Benchmark").to_frame("annual_ret")
    group_annual_ret["group"] = list(range(1, g + 1))
    corr_value = round(group_annual_ret.corr(method="spearman").iloc[0, 1], 4)
    group_annual_ret.annual_ret.plot(
        kind="bar", figsize=(10, 5), title=f"{name}_åˆ†å±‚è¶…é¢å¹´åŒ–æ”¶ç›Š_å•è°ƒæ€§{corr_value}"
    )

    # å‡€å€¼è¡¨ç°å›¾ - ä¼˜åŒ–å›¾ä¾‹æ˜¾ç¤º
    ax = group_return.plot(figsize=(10, 5), title=f"{name}_åˆ†å±‚å‡€å€¼è¡¨ç°")
    ax.legend(bbox_to_anchor=(1.05, 1), loc="upper left", ncol=1, fontsize=8)

    yby_performance = (
        group_return.pct_change()
        .resample("Y")
        .apply(lambda x: (1 + x).cumprod().iloc[-1])
        .T
    )
    yby_performance -= yby_performance.loc["Benchmark"]
    yby_performance = yby_performance.replace(0, np.nan).dropna(how="all")

    # å®šä¹‰ä¸°å¯Œçš„é¢œè‰²è°ƒè‰²æ¿
    colors = [
        "#FF6B6B",  # çŠç‘šçº¢
        "#4ECDC4",  # é’ç»¿è‰²
        "#45B7D1",  # å¤©è“è‰²
        "#96CEB4",  # è–„è·ç»¿
        "#FFEAA7",  # æµ…é»„è‰²
        "#DDA0DD",  # æ¢…èŠ±è‰²
        "#98D8C8",  # è–„è·è“
        "#F7DC6F",  # é‡‘é»„è‰²
        "#BB8FCE",  # æ·¡ç´«è‰²
        "#85C1E9",  # æµ…è“è‰²
    ]

    # é€å¹´åˆ†å±‚å¹´åŒ–æ”¶ç›Šå›¾ - ä¼˜åŒ–é¢œè‰²å’Œå›¾ä¾‹
    ax = yby_performance.plot(
        kind="bar",
        figsize=(12, 6),
        title=f"{name}_é€å¹´åˆ†å±‚å¹´åŒ–æ”¶ç›Š",
        color=colors[: len(yby_performance.columns)],
    )
    # è®¾ç½®å›¾ä¾‹ä¸ºæ°´å¹³æ’åˆ—ï¼Œä½ç½®åœ¨å›¾ä¸‹æ–¹
    ax.legend(bbox_to_anchor=(0.5, -0.15), loc="upper center", ncol=5, fontsize=9)

    return group_return, turnover_ratio


# æ•°æ®æ¸…æ´—å°è£…å‡½æ•°ï¼šåˆ¸æ± æ¸…æ´—ã€ç¦»ç¾¤å€¼å¤„ç†ã€æ ‡å‡†åŒ–å¤„ç†ã€ä¸­æ€§åŒ–å¤„ç†ã€æ¶¨åœè¿‡æ»¤
def preprocess_factor(factor, stock_universe, index_item):

    stock_list = stock_universe.columns.tolist()
    date_list = stock_universe.index.tolist()
    start_date = date_list[0].strftime("%F")
    end_date = date_list[-1].strftime("%F")

    # å°è¯•ä»æ–°è·¯å¾„è¯»å–combo_mask
    new_path = get_data_path('combo_mask', 
                            index_item=index_item, 
                            start_date=start_date, 
                            end_date=end_date)
    legacy_path = get_legacy_path(f"combo_mask_{index_item}_{start_date}_{end_date}.pkl")
    
    combo_mask = None
    
    # ä¼˜å…ˆå°è¯•æ–°è·¯å¾„
    try:
        combo_mask = pd.read_pickle(new_path)
        print(f"âœ… ä»æ–°è·¯å¾„åŠ è½½combo_mask: {new_path}")
    except:
        # å°è¯•æ—§è·¯å¾„
        try:
            combo_mask = pd.read_pickle(legacy_path)
            print(f"âš ï¸  ä»æ—§è·¯å¾„åŠ è½½combo_mask: {legacy_path}")
            # è‡ªåŠ¨è¿ç§»åˆ°æ–°è·¯å¾„
            combo_mask.to_pickle(new_path)
            print(f"âœ… å·²è¿ç§»combo_maskåˆ°æ–°è·¯å¾„: {new_path}")
        except:
            print(f"ğŸ“ è®¡ç®—æ–°çš„combo_mask...")
            pass
    
    # å¦‚æœéƒ½æ²¡æœ‰æ‰¾åˆ°ï¼Œåˆ™é‡æ–°è®¡ç®—
    if combo_mask is None:
        #  æ–°è‚¡è¿‡æ»¤
        new_stock_filter = get_new_stock_filter(stock_list, date_list)
        # stè¿‡æ»¤
        st_filter = get_st_filter(stock_list, date_list)
        # åœç‰Œè¿‡æ»¤
        suspended_filter = get_suspended_filter(stock_list, date_list)

        combo_mask = (
            new_stock_filter.astype(int)
            + st_filter.astype(int)
            + suspended_filter.astype(int)
            + (~stock_universe).astype(int)
        ) == 0

        # ä¿å­˜åˆ°æ–°è·¯å¾„
        combo_mask.to_pickle(new_path)
        print(f"ğŸ’¾ combo_maskå·²ä¿å­˜åˆ°: {new_path}")

    # axis=1,è¿‡æ»¤æ‰æ‰€æœ‰æ—¥æœŸæˆªé¢éƒ½æ˜¯nançš„è‚¡ç¥¨
    factor = factor.mask(~combo_mask).dropna(axis=1, how="all")

    # ç¦»ç¾¤å€¼å¤„ç†
    factor = mad_vectorized(factor)

    # æ ‡å‡†åŒ–å¤„ç†
    factor = standardize(factor)

    # ä¸­æ€§åŒ–å¤„ç†
    factor = neutralization_vectorized(factor, stock_list)

    # æ¶¨åœè¿‡æ»¤
    limit_up_filter = get_limit_up_filter(stock_list, date_list)
    factor = factor.mask(limit_up_filter)

    return factor


# æ•°æ®æ¸…æ´—å°è£…å‡½æ•°ï¼šåˆ¸æ± æ¸…æ´—ã€æ¶¨åœè¿‡æ»¤
def preprocess_factor_without_neutralization(factor, stock_universe, index_item):

    stock_list = stock_universe.columns.tolist()
    date_list = stock_universe.index.tolist()
    start_date = date_list[0].strftime("%F")
    end_date = date_list[-1].strftime("%F")

    # å°è¯•ä»æ–°è·¯å¾„è¯»å–combo_mask
    new_path = get_data_path('combo_mask', 
                            index_item=index_item, 
                            start_date=start_date, 
                            end_date=end_date)
    legacy_path = get_legacy_path(f"combo_mask_{index_item}_{start_date}_{end_date}.pkl")
    
    combo_mask = None
    
    # ä¼˜å…ˆå°è¯•æ–°è·¯å¾„
    try:
        combo_mask = pd.read_pickle(new_path)
        print(f"âœ… ä»æ–°è·¯å¾„åŠ è½½combo_mask: {new_path}")
    except:
        # å°è¯•æ—§è·¯å¾„
        try:
            combo_mask = pd.read_pickle(legacy_path)
            print(f"âš ï¸  ä»æ—§è·¯å¾„åŠ è½½combo_mask: {legacy_path}")
            # è‡ªåŠ¨è¿ç§»åˆ°æ–°è·¯å¾„
            combo_mask.to_pickle(new_path)
            print(f"âœ… å·²è¿ç§»combo_maskåˆ°æ–°è·¯å¾„: {new_path}")
        except:
            print(f"ğŸ“ è®¡ç®—æ–°çš„combo_mask...")
            pass
    
    # å¦‚æœéƒ½æ²¡æœ‰æ‰¾åˆ°ï¼Œåˆ™é‡æ–°è®¡ç®—
    if combo_mask is None:
        #  æ–°è‚¡è¿‡æ»¤
        new_stock_filter = get_new_stock_filter(stock_list, date_list)
        # stè¿‡æ»¤
        st_filter = get_st_filter(stock_list, date_list)
        # åœç‰Œè¿‡æ»¤
        suspended_filter = get_suspended_filter(stock_list, date_list)

        combo_mask = (
            new_stock_filter.astype(int)
            + st_filter.astype(int)
            + suspended_filter.astype(int)
            + (~stock_universe).astype(int)
        ) == 0

        # ä¿å­˜åˆ°æ–°è·¯å¾„
        combo_mask.to_pickle(new_path)
        print(f"ğŸ’¾ combo_maskå·²ä¿å­˜åˆ°: {new_path}")

    # axis=1,è¿‡æ»¤æ‰æ‰€æœ‰æ—¥æœŸæˆªé¢éƒ½æ˜¯nançš„è‚¡ç¥¨
    factor = factor.mask(~combo_mask).dropna(axis=1, how="all")

    # æ¶¨åœè¿‡æ»¤
    limit_up_filter = get_limit_up_filter(stock_list, date_list)
    factor = factor.mask(limit_up_filter)

    return factor


def factor_layered_backtest(df, n, g, index_item, name="", rebalance=False):
    """
    å› å­åˆ†å±‚å›æµ‹å‡½æ•°ï¼ˆä¼˜åŒ–ç‰ˆï¼‰

    :param df: å› å­å€¼ DataFrame (unstackæ ¼å¼)
    :param n: è°ƒä»“é¢‘ç‡ï¼ˆå¤©æ•°ï¼‰
    :param g: åˆ†ç»„æ•°é‡
    :param index_item: åˆ¸æ± åç§°
    :param name: å› å­åç§°
    :param rebalance: æ˜¯å¦æ¯æ—¥é‡æ–°å¹³è¡¡æƒé‡
        - True: æ¯æ—¥ç­‰æƒé‡å¹³å‡ï¼ˆé«˜äº¤æ˜“æˆæœ¬ï¼‰
        - False: ä¹°å…¥æŒæœ‰ç­–ç•¥ï¼ˆä½äº¤æ˜“æˆæœ¬ï¼‰
    :return: (group_return, turnover_ratio)
        - group_return: å„åˆ†ç»„çš„ç´¯è®¡å‡€å€¼è¡¨ç°
        - turnover_ratio: å„åˆ†ç»„çš„æ¢æ‰‹ç‡
    """

    df = df.shift(1).iloc[1:]
    order_book_ids = df.columns.tolist()
    datetime_period = df.index
    start = datetime_period.min().strftime("%F")
    end = datetime_period.max().strftime("%F")

    # å°è¯•ä»æ–°è·¯å¾„è¯»å–return_1d
    new_path = get_data_path('return_1d', index_item=index_item, start=start, end=end)
    legacy_path = get_legacy_path(f"return_1d_{index_item}_{start}_{end}.pkl")
    
    return_1d = None
    
    # ä¼˜å…ˆå°è¯•æ–°è·¯å¾„
    try:
        return_1d = pd.read_pickle(new_path)
        print(f"âœ… ä»æ–°è·¯å¾„åŠ è½½return_1d: {new_path}")
    except:
        # å°è¯•æ—§è·¯å¾„
        try:
            return_1d = pd.read_pickle(legacy_path)
            print(f"âš ï¸  ä»æ—§è·¯å¾„åŠ è½½return_1d: {legacy_path}")
            # è‡ªåŠ¨è¿ç§»åˆ°æ–°è·¯å¾„
            return_1d.to_pickle(new_path)
            print(f"âœ… å·²è¿ç§»return_1dåˆ°æ–°è·¯å¾„: {new_path}")
        except:
            print(f"ğŸ“ è®¡ç®—æ–°çš„return_1d...")
            pass
    
    # å¦‚æœéƒ½æ²¡æœ‰æ‰¾åˆ°ï¼Œåˆ™é‡æ–°è®¡ç®—
    if return_1d is None:
        index_fix = INDEX_FIX(start, end, index_item)
        order_book_ids = index_fix.columns.tolist()
        open = get_price(
            order_book_ids,
            start,
            get_next_trading_date(end, 1),
            "1d",
            "open",
            "pre",
            False,
            True,
        ).open.unstack("order_book_id")
        return_1d = open.pct_change().shift(-1).dropna(axis=0, how="all").stack()
        
        # ä¿å­˜åˆ°æ–°è·¯å¾„
        return_1d.to_pickle(new_path)
        print(f"ğŸ’¾ return_1då·²ä¿å­˜åˆ°: {new_path}")

    # æ•°æ®åˆå¹¶ï¼Œä½¿ç”¨multiindex
    factor_data = df.stack().to_frame("factor")
    factor_data["current_return"] = return_1d
    factor_data = factor_data.dropna()

    # è·å–è°ƒä»“æ—¥æœŸå’Œæ•°æ®è¾¹ç•Œ
    actual_rebalance_dates = datetime_period[::n]  # çœŸæ­£çš„è°ƒä»“æ—¥æœŸ
    # data_end_date = datetime_period[-1]  # æ•°æ®ç»“æŸè¾¹ç•Œ

    # æ‰¹é‡åˆ†ç»„
    all_groups = {}
    turnover_data = []

    ##########è®¡ç®—è°ƒä»“æ—¥åˆ†ç»„å’Œæ¢æ‰‹ç‡##########
    # ä¸ºæ‰€æœ‰è°ƒä»“æ—¥æœŸæ„å»ºåˆ†ç»„ä¿¡æ¯
    for i, date in enumerate(actual_rebalance_dates):
        # è·å–å½“å‰å› å­å€¼
        current_factors = factor_data.loc[date, "factor"]

        # ä½¿ç”¨qcutè¿›è¡Œåˆ†ç»„
        groups = pd.qcut(current_factors, g, labels=range(1, g + 1))
        current_groups = (
            current_factors.groupby(groups).apply(lambda x: x.index.tolist()).to_dict()
        )

        # è®¡ç®—æ¢æ‰‹ç‡ï¼ˆé™¤äº†ç¬¬ä¸€æ¬¡ï¼‰
        if i > 0:
            turnover_rates = []
            for group_id in range(1, g + 1):
                if group_id in all_groups[i - 1] and group_id in current_groups:
                    old_stocks = set(all_groups[i - 1][group_id])
                    new_stocks = set(current_groups[group_id])
                    turnover = (
                        len(old_stocks - new_stocks) / len(old_stocks)
                        if old_stocks
                        else 0
                    )
                    turnover_rates.append(turnover)
                else:
                    turnover_rates.append(np.nan)

            turnover_data.append({"date": date, "turnover": turnover_rates})

        all_groups[i] = current_groups

    if turnover_data:
        turnover_ratio = pd.DataFrame(
            [d["turnover"] for d in turnover_data],
            index=[d["date"] for d in turnover_data],
            columns=[f"G{i}" for i in range(1, g + 1)],
        )
    else:
        turnover_ratio = pd.DataFrame()

    ##########è®¡ç®—åˆ†ç»„æ”¶ç›Š##########
    group_returns_list = []

    # å¤„ç†æ‰€æœ‰è°ƒä»“å‘¨æœŸï¼ˆå‘¨æœŸå«ä¹‰ï¼šè°ƒä»“æ—¥ï¼‰
    for i, start_date in enumerate(actual_rebalance_dates):

        # ç¡®å®šå‘¨æœŸç»“æŸæ—¥æœŸ
        is_last_period = i == len(actual_rebalance_dates) - 1

        if not is_last_period:
            # æ­£å¸¸å‘¨æœŸï¼šåˆ°ä¸‹ä¸€ä¸ªè°ƒä»“æ—¥
            end_date = actual_rebalance_dates[i + 1]
            period_mask = (factor_data.index.get_level_values(0) >= start_date) & (
                factor_data.index.get_level_values(0) < end_date
            )
        else:
            # æœ€åä¸€ä¸ªå‘¨æœŸï¼šåˆ°æ•°æ®ç»“æŸæ—¥
            period_mask = factor_data.index.get_level_values(0) >= start_date

        period_data = factor_data[period_mask]

        if i not in all_groups:
            continue

        group_dict = all_groups[i]
        period_returns = {}

        for group_id in range(1, g + 1):
            if group_id not in group_dict:
                continue

            stocks = group_dict[group_id]
            # è·å–è¯¥ç»„è‚¡ç¥¨çš„æ”¶ç›Šç‡æ•°æ®
            group_data = period_data[period_data.index.get_level_values(1).isin(stocks)]

            if len(group_data) == 0:
                continue

            # æ ¹æ®rebalanceå‚æ•°é€‰æ‹©è®¡ç®—æ–¹å¼ï¼Œæœ‰æ¯æ—¥é‡æ–°å¹³è¡¡å’Œä¹°å…¥æŒæœ‰ä¸¤ç§æ–¹å¼
            if rebalance:
                # æ¯æ—¥ç­‰æƒé‡å¹³å‡ï¼ˆæ¯å¤©é‡æ–°å¹³è¡¡æƒé‡ï¼‰
                portfolio_daily_returns = group_data.groupby(level=0)[
                    "current_return"
                ].mean()
            else:

                # å°†æ•°æ®é‡æ–°æ•´ç†ä¸ºæ—¥æœŸÃ—è‚¡ç¥¨çš„æ ¼å¼
                stock_daily_returns = group_data.reset_index().pivot(
                    index="datetime", columns="order_book_id", values="current_return"
                )
                # ç­‰æƒé‡ä¹°å…¥è‚¡ç¥¨ï¼Œä¸é‡æ–°å¹³è¡¡
                group_cum_returns = (1 + stock_daily_returns).cumprod().mean(axis=1)
                portfolio_daily_returns = group_cum_returns.pct_change()  # è½¬ä¸ºæ—¥æ”¶ç›Šç‡

                # å¤„ç†ç¬¬ä¸€å¤©çš„æ”¶ç›Šç‡
                if not portfolio_daily_returns.empty:
                    portfolio_daily_returns.iloc[0] = stock_daily_returns.iloc[0].mean()

            period_returns[f"G{group_id}"] = portfolio_daily_returns

        # å°†è¯¥æœŸé—´æ”¶ç›Šç‡æ·»åŠ åˆ°æ€»åˆ—è¡¨ä¸­
        if period_returns:
            period_df = pd.DataFrame(period_returns)
            group_returns_list.append(period_df)

    if group_returns_list:
        group_return = pd.concat(group_returns_list, axis=0)
    else:
        group_return = pd.DataFrame()

    if not group_return.empty:
        # åŸºå‡†æ”¶ç›Š
        group_return["Benchmark"] = group_return.mean(axis=1)
        group_return = (group_return + 1).cumprod()

        # è®¡ç®—å¹´åŒ–æ”¶ç›Šç‡
        group_annual_ret = group_return.iloc[-1] ** (252 / len(group_return)) - 1
        group_annual_ret = group_annual_ret - group_annual_ret.Benchmark
        group_annual_ret = group_annual_ret.drop("Benchmark").to_frame("annual_ret")
        group_annual_ret["group"] = list(range(1, g + 1))
        corr_value = round(group_annual_ret.corr(method="spearman").iloc[0, 1], 4)

        group_annual_ret.annual_ret.plot(
            kind="bar",
            figsize=(10, 5),
            title=f"{name}_åˆ†å±‚è¶…é¢å¹´åŒ–æ”¶ç›Š_å•è°ƒæ€§{corr_value}",
        )

        # å‡€å€¼è¡¨ç°å›¾ - çªå‡ºG1å’ŒG10ç»„ï¼Œå¼±åŒ–å…¶ä»–ç»„
        fig, ax = plt.subplots(figsize=(10, 5))

        # ç»˜åˆ¶æ‰€æœ‰ç»„çš„çº¿æ¡
        for col in group_return.columns:
            if col in ["G1", "G10"]:
                # çªå‡ºæ˜¾ç¤ºG1å’ŒG10ç»„
                linewidth = 3
                alpha = 1.0
                if col == "G10":
                    color = "#FF0000"  # é²œçº¢è‰²
                else:  # G1
                    color = "#00AA00"  # é²œç»¿è‰²
            elif col == "Benchmark":
                # åŸºå‡†çº¿ä¿æŒå¯è§
                linewidth = 2
                alpha = 0.8
                color = "#000000"  # é»‘è‰²
            else:
                # å¼±åŒ–å…¶ä»–ç»„
                linewidth = 1
                alpha = 0.3
                color = "#CCCCCC"  # æµ…ç°è‰²

            ax.plot(
                group_return.index,
                group_return[col],
                label=col,
                linewidth=linewidth,
                alpha=alpha,
                color=color,
            )

        ax.set_title(f"{name}_åˆ†å±‚å‡€å€¼è¡¨ç°")
        ax.legend(bbox_to_anchor=(1.05, 1), loc="upper left", ncol=1, fontsize=8)
        ax.grid(True, alpha=0.3)

        # å¹´åŒ–æ”¶ç›Šå›¾
        yby_performance = (
            group_return.pct_change()
            .resample("Y")
            .apply(lambda x: (1 + x).cumprod().iloc[-1])
            .T
        )
        yby_performance -= yby_performance.loc["Benchmark"]
        yby_performance = yby_performance.replace(0, np.nan).dropna(how="all")

        # æ ¹æ®å¹´ä»½æ•°é‡åŠ¨æ€ç”Ÿæˆé¢œè‰²
        num_years = len(yby_performance.columns)

        # åŸºç¡€é«˜å¯¹æ¯”åº¦é¢œè‰²æ–¹æ¡ˆ
        base_colors = [
            "#1f77b4",  # è“è‰²
            "#ff7f0e",  # æ©™è‰²
            "#2ca02c",  # ç»¿è‰²
            "#d62728",  # çº¢è‰²
            "#9467bd",  # ç´«è‰²
            "#8c564b",  # æ£•è‰²
            "#e377c2",  # ç²‰è‰²
            "#7f7f7f",  # ç°è‰²
            "#bcbd22",  # æ©„æ¦„è‰²
            "#000080",  # æ·±è“è‰²
            "#FF1493",  # æ·±ç²‰è‰²
            "#00CED1",  # æš—é’è‰²
            "#FF4500",  # æ©™çº¢è‰²
            "#32CD32",  # é…¸æ©™ç»¿
            "#8A2BE2",  # è“ç´«è‰²
            "#DC143C",  # æ·±çº¢è‰²
            "#00BFFF",  # æ·±å¤©è“
            "#FFD700",  # é‡‘è‰²
            "#FF69B4",  # çƒ­ç²‰è‰²
            "#228B22",  # æ£®æ—ç»¿
        ]

        # å¦‚æœå¹´ä»½æ•°é‡è¶…è¿‡åŸºç¡€é¢œè‰²æ•°é‡ï¼Œä½¿ç”¨matplotlibçš„é¢œè‰²å¾ªç¯
        if num_years > len(base_colors):
            colors = [plt.cm.tab20(i / num_years) for i in range(num_years)]
        else:
            colors = base_colors[:num_years]

        # é€å¹´åˆ†å±‚å¹´åŒ–æ”¶ç›Šå›¾
        ax = yby_performance.plot(
            kind="bar",
            figsize=(12, 6),
            title=f"{name}_é€å¹´åˆ†å±‚å¹´åŒ–æ”¶ç›Š",
            color=colors[: len(yby_performance.columns)],
        )
        # è®¾ç½®å›¾ä¾‹ä¸ºæ°´å¹³æ’åˆ—ï¼Œä½ç½®åœ¨å›¾ä¸‹æ–¹
        ax.legend(bbox_to_anchor=(0.5, -0.15), loc="upper center", ncol=5, fontsize=9)

    return group_return, turnover_ratio


# è·å–ä¹°å…¥é˜Ÿåˆ—
def get_buy_list(df, top_type="rank", rank_n=100, quantile_q=0.8):
    """
    :param df: å› å­å€¼ -> dataframe/unstack
    :param top_type: é€‰æ‹©ä¹°å…¥é˜Ÿåˆ—æ–¹å¼ï¼Œä»['rank','quantile']é€‰æ‹©ä¸€ç§æ–¹å¼ -> str
    :param rank_n: å€¼æœ€å¤§çš„å‰nåªçš„è‚¡ç¥¨ -> int
    :param quantile_q: å€¼æœ€å¤§çš„å‰nåˆ†ä½æ•°çš„è‚¡ç¥¨ -> float
    :return df: ä¹°å…¥é˜Ÿåˆ— -> dataframe/unstack
    """

    if top_type == "rank":
        df = df.rank(axis=1, ascending=False) <= rank_n
    elif top_type == "quantile":
        df = df.sub(df.quantile(quantile_q, axis=1), axis=0) > 0
    else:
        print("select one from ['rank','quantile']")

    df = df.astype(int)
    df = df.replace(0, np.nan).dropna(how="all", axis=1)

    return df


# 4.2 è·å–æ ‡çš„æ”¶ç›Š
def get_bar(df, adjust):
    """
    :param df: ä¹°å…¥é˜Ÿåˆ— -> dataframe/unstack
    :return ret: åŸºå‡†çš„é€æ—¥æ”¶ç›Š -> dataframe
    """
    start_date = get_previous_trading_date(df.index.min(), 1).strftime("%F")
    end_date = df.index.max().strftime("%F")
    stock_list = df.columns.tolist()
    price_open = get_price(
        stock_list, start_date, end_date, fields=["open"], adjust_type=adjust
    ).open.unstack("order_book_id")

    return price_open


# 4.3 è·å–åŸºå‡†æ”¶ç›Š
def get_benchmark(df, benchmark, benchmark_type="mcw"):
    """
    :param df: ä¹°å…¥é˜Ÿåˆ— -> dataframe/unstack
    :param benchmark: åŸºå‡†æŒ‡æ•° -> str
    :return ret: åŸºå‡†çš„é€æ—¥æ”¶ç›Š -> dataframe
    """
    start_date = get_previous_trading_date(df.index.min(), 1).strftime("%F")
    end_date = df.index.max().strftime("%F")
    if benchmark_type == "mcw":
        price_open = get_price(
            [benchmark], start_date, end_date, fields=["open"]
        ).open.unstack("order_book_id")
    else:
        index_fix = INDEX_FIX(start_date, end_date, benchmark)
        stock_list = index_fix.columns.tolist()
        price_open = get_price(
            stock_list, start_date, end_date, fields=["open"]
        ).open.unstack("order_book_id")
        price_open = price_open.pct_change().mask(~index_fix).mean(axis=1)
        price_open = (1 + price_open).cumprod().to_frame(benchmark)

    return price_open


# 4.4 å›æµ‹æ¡†æ¶
def backtest(
    portfolio_weights,
    rebalance_frequency=20,
    initial_capital=10000 * 10000,
    stamp_tax_rate=0.0005,
    transfer_fee_rate=0.0001,
    commission_rate=0.0002,
    min_transaction_fee=5,
    cash_annual_yield=0.02,
):
    """
    é‡åŒ–ç­–ç•¥å›æµ‹æ¡†æ¶

    :param portfolio_weights: æŠ•èµ„ç»„åˆæƒé‡çŸ©é˜µ -> DataFrame
    :param rebalance_frequency: è°ƒä»“é¢‘ç‡ï¼ˆå¤©æ•°ï¼‰ -> int
    :param initial_capital: åˆå§‹èµ„æœ¬ -> float
    :param stamp_tax_rate: å°èŠ±ç¨ç‡ -> float
    :param transfer_fee_rate: è¿‡æˆ·è´¹ç‡ -> float
    :param commission_rate: ä½£é‡‘ç‡ -> float
    :param min_transaction_fee: æœ€ä½äº¤æ˜“æ‰‹ç»­è´¹ -> float
    :param cash_annual_yield: ç°é‡‘å¹´åŒ–æ”¶ç›Šç‡ -> float
    :return: è´¦æˆ·å†å²è®°å½• -> DataFrame
    """

    # =========================== åŸºç¡€å‚æ•°åˆå§‹åŒ– ===========================
    # ä¿å­˜åˆå§‹èµ„æœ¬å¤‡ä»½ï¼Œç”¨äºæœ€åçš„ç»Ÿè®¡è®¡ç®—
    cash = initial_capital
    # åˆå§‹åŒ–å†å²æŒä»“ï¼Œç¬¬ä¸€æ¬¡è°ƒä»“æ—¶ä¸º0
    previous_holdings = 0
    # ä¹°å…¥æˆæœ¬è´¹ç‡ï¼šè¿‡æˆ·è´¹ + ä½£é‡‘
    buy_cost_rate = transfer_fee_rate + commission_rate
    # å–å‡ºæˆæœ¬è´¹ç‡ï¼šå°èŠ±ç¨ + è¿‡æˆ·è´¹ + ä½£é‡‘
    sell_cost_rate = stamp_tax_rate + transfer_fee_rate + commission_rate
    # ç°é‡‘è´¦æˆ·æ—¥åˆ©ç‡ï¼ˆå¹´åŒ–æ”¶ç›Šç‡è½¬æ¢ä¸ºæ—¥æ”¶ç›Šç‡ï¼‰
    daily_cash_yield = (1 + cash_annual_yield) ** (1 / 252) - 1

    # =========================== æ•°æ®ç»“æ„åˆå§‹åŒ– ===========================
    # åˆ›å»ºè´¦æˆ·å†å²è®°å½•è¡¨ï¼Œç´¢å¼•ä¸ºæ‰€æœ‰äº¤æ˜“æ—¥
    account_history = pd.DataFrame(
        index=portfolio_weights.index,
        # åˆ—1ï¼šè´¦æˆ·æ€»èµ„äº§
        # åˆ—2ï¼šæŒä»“å¸‚å€¼
        # åˆ—3ï¼šç°é‡‘è´¦æˆ·ä½™é¢
        columns=["total_account_asset", "holding_market_cap", "cash_account"],
    )
    # è·å–æ‰€æœ‰è‚¡ç¥¨çš„å¼€ç›˜ä»·æ ¼æ•°æ®ï¼ˆæœªå¤æƒï¼‰
    open_prices = get_bar(portfolio_weights, "none")
    # è·å–æ‰€æœ‰è‚¡ç¥¨çš„åå¤æƒä»·æ ¼æ•°æ®
    adjusted_prices = get_bar(portfolio_weights, "post")
    # è·å–æ¯åªè‚¡ç¥¨çš„æœ€å°äº¤æ˜“å•ä½ï¼ˆé€šå¸¸ä¸º100è‚¡ï¼‰
    min_trade_units = pd.Series(
        dict(
            [
                (stock, instruments(stock).round_lot)
                for stock in portfolio_weights.columns.tolist()
            ]
        )
    )
    # ç”Ÿæˆè°ƒä»“æ—¥æœŸåˆ—è¡¨ï¼šæ¯ rebalance_frequency å¤©è°ƒä»“ä¸€æ¬¡
    # ç¡®ä¿æœ€åä¸€å¤©ä¹Ÿè¢«åŒ…å«åœ¨è°ƒä»“æ—¥ä¸­
    rebalance_dates = sorted(
        set(
            portfolio_weights.index.tolist()[::rebalance_frequency]
            + [portfolio_weights.index[-1]]
        )
    )

    # =========================== å¼€å§‹é€æœŸè°ƒä»“å¾ªç¯ ===========================
    for i in tqdm(range(0, len(rebalance_dates) - 1)):
        rebalance_date = rebalance_dates[i]  # å½“å‰è°ƒä»“æ—¥æœŸ
        next_rebalance_date = rebalance_dates[i + 1]  # ä¸‹ä¸€ä¸ªè°ƒä»“æ—¥æœŸ

        # =========================== è·å–å½“å‰è°ƒä»“æ—¥çš„ç›®æ ‡æƒé‡ ===========================
        # è·å–å½“å‰è°ƒä»“æ—¥çš„ç›®æ ‡æƒé‡ï¼Œå¹¶åˆ é™¤ç¼ºå¤±å€¼
        current_target_weights = portfolio_weights.loc[rebalance_date].dropna()
        # è·å–ç›®æ ‡è‚¡ç¥¨åˆ—è¡¨
        target_stocks = current_target_weights.index.tolist()

        # =========================== è®¡ç®—ç›®æ ‡æŒä»“æ•°é‡ ===========================
        # è®¡ç®—å…¬å¼ï¼šç›®æ ‡æŒä»“ = å‘ä¸‹å–æ•´(æƒé‡ * å¯ç”¨èµ„é‡‘ / (è°ƒæ•´åä»·æ ¼ * æœ€å°äº¤æ˜“å•ä½)) * æœ€å°äº¤æ˜“å•ä½
        # æ­¥éª¤1ï¼šæŒ‰æƒé‡åˆ†é…èµ„é‡‘
        # æ­¥éª¤2ï¼šä¹˜ä»¥å¯ç”¨èµ„é‡‘
        # æ­¥éª¤3ï¼šé™¤ä»¥è°ƒæ•´åçš„è‚¡ä»·ï¼ˆé¢„ç•™å–å‡ºæ‰‹ç»­è´¹ï¼‰
        # æ­¥éª¤4ï¼šå‘ä¸‹å–æ•´åˆ°æœ€å°äº¤æ˜“å•ä½çš„æ•´æ•°å€
        # æ­¥éª¤5ï¼šè½¬æ¢ä¸ºå®é™…å¯äº¤æ˜“çš„è‚¡æ•°
        target_holdings = (
            current_target_weights
            * cash
            / (open_prices.loc[rebalance_date, target_stocks] * (1 + sell_cost_rate))
            // min_trade_units.loc[target_stocks]
        ) * min_trade_units.loc[target_stocks]

        # =========================== ä»“ä½å˜åŠ¨è®¡ç®— ===========================
        ## æ­¥éª¤1ï¼šè®¡ç®—æŒä»“å˜åŠ¨é‡ï¼ˆç›®æ ‡æŒä»“ - å†å²æŒä»“ï¼‰
        # fill_value=0 ç¡®ä¿æ–°å¢è‚¡ç¥¨ï¼ˆå†å²æŒä»“ä¸ºç©ºï¼‰å’Œæ¸…ä»“è‚¡ç¥¨ï¼ˆç›®æ ‡æŒä»“ä¸ºç©ºï¼‰éƒ½èƒ½æ­£ç¡®è®¡ç®—
        holdings_change_raw = target_holdings.sub(
            previous_holdings, fill_value=0
        )  # è®¡ç®—åŸå§‹æŒä»“å˜åŠ¨é‡

        ## æ­¥éª¤2ï¼šè¿‡æ»¤æ‰æ— å˜åŠ¨çš„è‚¡ç¥¨ï¼ˆå˜åŠ¨é‡ä¸º0çš„è‚¡ç¥¨ï¼‰
        # å°†å˜åŠ¨é‡ä¸º0çš„è‚¡ç¥¨æ ‡è®°ä¸ºNaNï¼Œç„¶ååˆ é™¤ï¼Œåªä¿ç•™éœ€è¦è°ƒä»“çš„è‚¡ç¥¨
        holdings_change_filtered = holdings_change_raw.replace(
            0, np.nan
        )  # å°†æ— å˜åŠ¨çš„è‚¡ç¥¨æ ‡è®°ä¸ºNaN

        ## æ­¥éª¤3ï¼šè·å–æœ€ç»ˆçš„äº¤æ˜“æ‰§è¡Œåˆ—è¡¨
        # æ­£æ•°è¡¨ç¤ºéœ€è¦ä¹°å…¥çš„è‚¡æ•°ï¼Œè´Ÿæ•°è¡¨ç¤ºéœ€è¦å–å‡ºçš„è‚¡æ•°
        trades_to_execute = (
            holdings_change_filtered.dropna()
        )  # åˆ é™¤NaNï¼Œåªä¿ç•™éœ€è¦æ‰§è¡Œçš„äº¤æ˜“

        # =========================== è·å–å½“å‰äº¤æ˜“æ—¥ä»·æ ¼ ===========================
        current_prices = open_prices.loc[
            rebalance_date
        ]  # è·å–å½“å‰è°ƒä»“æ—¥çš„æ‰€æœ‰è‚¡ç¥¨å¼€ç›˜ä»·

        # =========================== è®¡ç®—äº¤æ˜“æˆæœ¬ ===========================
        def calc_transaction_fee(transaction_value, min_transaction_fee):
            """
            è®¡ç®—å•ç¬”äº¤æ˜“çš„æ‰‹ç»­è´¹
            :param transaction_value: äº¤æ˜“é‡‘é¢ï¼ˆæ­£æ•°ä¸ºä¹°å…¥ï¼Œè´Ÿæ•°ä¸ºå–å‡ºï¼‰
            :param min_transaction_fee: æœ€ä½äº¤æ˜“æ‰‹ç»­è´¹
            :return: äº¤æ˜“æ‰‹ç»­è´¹
            """
            if pd.isna(transaction_value) or transaction_value == 0:
                return 0  # æ— äº¤æ˜“æ—¶æ‰‹ç»­è´¹ä¸º0
            elif transaction_value < 0:  # å–å‡ºäº¤æ˜“ï¼ˆè´Ÿæ•°ï¼‰
                fee = (
                    -transaction_value * sell_cost_rate
                )  # å–å‡ºæ‰‹ç»­è´¹ï¼šå°èŠ±ç¨ + è¿‡æˆ·è´¹ + ä½£é‡‘
            else:  # ä¹°å…¥äº¤æ˜“ï¼ˆæ­£æ•°ï¼‰
                fee = transaction_value * buy_cost_rate  # ä¹°å…¥æ‰‹ç»­è´¹ï¼šè¿‡æˆ·è´¹ + ä½£é‡‘

            # åº”ç”¨æœ€ä½æ‰‹ç»­è´¹é™åˆ¶
            return max(fee, min_transaction_fee)  # è¿”å›å®é™…æ‰‹ç»­è´¹å’Œæœ€ä½æ‰‹ç»­è´¹ä¸­çš„è¾ƒå¤§å€¼

        # è®¡ç®—æ€»äº¤æ˜“æˆæœ¬ï¼šäº¤æ˜“é‡‘é¢ = ä»·æ ¼ * äº¤æ˜“è‚¡æ•°
        # è®¡ç®—æ¯ç¬”äº¤æ˜“çš„äº¤æ˜“é‡‘é¢
        # å¯¹æ¯ç¬”äº¤æ˜“è®¡ç®—æ‰‹ç»­è´¹
        # æ±‚å’Œå¾—åˆ°æ€»æ‰‹ç»­è´¹
        total_transaction_cost = (
            (current_prices * trades_to_execute)
            .apply(lambda x: calc_transaction_fee(x, min_transaction_fee))
            .sum()
        )

        # =========================== ä»·æ ¼å¤æƒè°ƒæ•´ ===========================
        # è®¡ç®—ä»è°ƒä»“æ—¥åˆ°ä¸‹ä¸€è°ƒä»“æ—¥çš„ä»·æ ¼å¤æƒæ¯”ç‡
        price_adjustment_ratio = (
            adjusted_prices.loc[rebalance_date:next_rebalance_date]
            / adjusted_prices.loc[rebalance_date]
        )  # å¤æƒæ¯”ç‡ = æœŸé—´ä»·æ ¼ / èµ·å§‹ä»·æ ¼

        # å°†å¤æƒæ¯”ç‡åº”ç”¨åˆ°å½“æ—¥å¼€ç›˜ä»·ï¼Œå¾—åˆ°æœŸé—´è°ƒæ•´åä»·æ ¼
        period_adjusted_prices = (
            price_adjustment_ratio.T.mul(  # è½¬ç½®ä»¥ä¾¿äºè®¡ç®—
                current_prices, axis=0
            )  # ä¹˜ä»¥å½“æ—¥å¼€ç›˜ä»·
            .dropna(how="all")
            .T
        )  # åˆ é™¤å…¨ä¸ºNaNçš„è¡Œå¹¶è½¬ç½®å›æ¥

        # =========================== è®¡ç®—æŠ•èµ„ç»„åˆå¸‚å€¼ ===========================
        # æŠ•èµ„ç»„åˆå¸‚å€¼ = æ¯åªè‚¡ç¥¨çš„(è°ƒæ•´åä»·æ ¼ * æŒä»“æ•°é‡)çš„æ€»å’Œ
        portfolio_market_value = (period_adjusted_prices * target_holdings).sum(
            axis=1
        )  # æŒ‰æ—¥è®¡ç®—æŠ•èµ„ç»„åˆå¸‚å€¼

        # =========================== è®¡ç®—ç°é‡‘è´¦æˆ·ä½™é¢ ===========================
        # åˆå§‹ç°é‡‘ä½™é¢ = å¯ç”¨èµ„é‡‘ - äº¤æ˜“æˆæœ¬ - åˆå§‹æŠ•èµ„é‡‘é¢
        initial_cash_balance = (
            cash - total_transaction_cost - portfolio_market_value.loc[rebalance_date]
        )

        # è®¡ç®—æœŸé—´ç°é‡‘è´¦æˆ·çš„å¤åˆ©å¢é•¿ï¼ˆæŒ‰æ—¥è®¡æ¯ï¼‰
        cash_balance = pd.Series(
            [
                initial_cash_balance
                * ((1 + daily_cash_yield) ** (day + 1))  # å¤åˆ©è®¡æ¯å…¬å¼
                for day in range(0, len(portfolio_market_value))
            ],  # å¯¹æ¯ä¸€å¤©è®¡ç®—
            index=portfolio_market_value.index,
        )  # ä½¿ç”¨ç›¸åŒçš„æ—¥æœŸç´¢å¼•

        # =========================== è®¡ç®—è´¦æˆ·æ€»èµ„äº§ ===========================
        total_portfolio_value = (
            portfolio_market_value + cash_balance
        )  # æ€»èµ„äº§ = æŒä»“å¸‚å€¼ + ç°é‡‘ä½™é¢

        # =========================== æ›´æ–°å†å²æ•°æ®ä¸ºä¸‹ä¸€æ¬¡è°ƒä»“åšå‡†å¤‡ ===========================
        previous_holdings = target_holdings  # æ›´æ–°å†å²æŒä»“ä¸ºå½“å‰ç›®æ ‡æŒä»“
        cash = total_portfolio_value.loc[
            next_rebalance_date
        ]  # æ›´æ–°å¯ç”¨èµ„é‡‘ä¸ºä¸‹ä¸€è°ƒä»“æ—¥çš„è´¦æˆ·æ€»å€¼

        # =========================== ä¿å­˜è´¦æˆ·å†å²è®°å½• ===========================
        # å°†å½“å‰æœŸé—´çš„è´¦æˆ·æ•°æ®ä¿å­˜åˆ°å†å²è®°å½•ä¸­ï¼ˆä¿ç–™2ä½å°æ•°ï¼‰
        account_history.loc[
            rebalance_date:next_rebalance_date, "total_account_asset"
        ] = round(total_portfolio_value, 2)
        account_history.loc[
            rebalance_date:next_rebalance_date, "holding_market_cap"
        ] = round(portfolio_market_value, 2)
        account_history.loc[rebalance_date:next_rebalance_date, "cash_account"] = round(
            cash_balance, 2
        )

    # =========================== æ·»åŠ åˆå§‹æ—¥è®°å½•å¹¶æ’åº ===========================
    # åœ¨ç¬¬ä¸€ä¸ªäº¤æ˜“æ—¥ä¹‹å‰æ·»åŠ åˆå§‹èµ„æœ¬è®°å½•
    initial_date = pd.to_datetime(
        get_previous_trading_date(account_history.index.min(), 1)
    )
    account_history.loc[initial_date] = [
        initial_capital,
        0,
        initial_capital,
    ]  # [æ€»èµ„äº§, æŒä»“å¸‚å€¼, ç°é‡‘ä½™é¢]
    account_history = account_history.sort_index()  # æŒ‰æ—¥æœŸæ’åº

    return account_history  # è¿”å›å®Œæ•´çš„è´¦æˆ·å†å²è®°å½•


# 4.5 å›æµ‹ç»©æ•ˆæŒ‡æ ‡ç»˜åˆ¶
def get_performance_analysis(account_result, rf=0.03, benchmark_index="000985.XSHG"):

    # åŠ å…¥åŸºå‡†
    performance = pd.concat(
        [
            account_result["total_account_asset"].to_frame("strategy"),
            get_benchmark(account_result, benchmark_index),
        ],
        axis=1,
    )
    performance_net = performance.pct_change().dropna(how="all")  # æ¸…ç®—è‡³å½“æ—¥å¼€ç›˜
    performance_cumnet = (1 + performance_net).cumprod()
    performance_cumnet["alpha"] = (
        performance_cumnet["strategy"] / performance_cumnet[benchmark_index]
    )
    performance_cumnet = performance_cumnet.fillna(1)

    # æŒ‡æ ‡è®¡ç®—
    performance_pct = performance_cumnet.pct_change().dropna()

    # ç­–ç•¥æ”¶ç›Š
    strategy_name, benchmark_name, alpha_name = performance_cumnet.columns.tolist()
    Strategy_Final_Return = performance_cumnet[strategy_name].iloc[-1] - 1

    # ç­–ç•¥å¹´åŒ–æ”¶ç›Š
    Strategy_Annualized_Return_EAR = (1 + Strategy_Final_Return) ** (
        252 / len(performance_cumnet)
    ) - 1

    # ç­–ç•¥å¹´åŒ–æ”¶ç›Š(ç®—æœ¯)
    Strategy_Annualized_Return_AM = performance_pct[strategy_name].mean() * 252

    # åŸºå‡†æ”¶ç›Š
    Benchmark_Final_Return = performance_cumnet[benchmark_name].iloc[-1] - 1

    # åŸºå‡†å¹´åŒ–æ”¶ç›Š
    Benchmark_Annualized_Return_EAR = (1 + Benchmark_Final_Return) ** (
        252 / len(performance_cumnet)
    ) - 1

    # alpha
    ols_result = sm.OLS(
        performance_pct[strategy_name] * 252 - rf,
        sm.add_constant(performance_pct[benchmark_name] * 252 - rf),
    ).fit()
    Alpha = ols_result.params[0]

    # beta
    Beta = ols_result.params[1]

    # beta_2 = np.cov(performance_pct[strategy_name],performance_pct[benchmark_name])[0,1]/performance_pct[benchmark_name].var()
    # æ³¢åŠ¨ç‡
    Strategy_Volatility = performance_pct[strategy_name].std() * np.sqrt(252)

    # å¤æ™®
    Strategy_Sharpe = (Strategy_Annualized_Return_EAR - rf) / Strategy_Volatility

    # å¤æ™®(ç®—æœ¯)
    Strategy_Sharpe_AM = (Strategy_Annualized_Return_AM - rf) / Strategy_Volatility

    # ä¸‹è¡Œæ³¢åŠ¨ç‡
    strategy_ret = performance_pct[strategy_name]
    Strategy_Down_Volatility = strategy_ret[strategy_ret < 0].std() * np.sqrt(252)

    # sortino
    Sortino = (Strategy_Annualized_Return_EAR - rf) / Strategy_Down_Volatility

    # è·Ÿè¸ªè¯¯å·®
    Tracking_Error = (
        performance_pct[strategy_name] - performance_pct[benchmark_name]
    ).std() * np.sqrt(252)

    # ä¿¡æ¯æ¯”ç‡
    Information_Ratio = (
        Strategy_Annualized_Return_EAR - Benchmark_Annualized_Return_EAR
    ) / Tracking_Error

    # æœ€å¤§å›æ’¤
    i = np.argmax(
        (
            np.maximum.accumulate(performance_cumnet[strategy_name])
            - performance_cumnet[strategy_name]
        )
        / np.maximum.accumulate(performance_cumnet[strategy_name])
    )
    j = np.argmax(performance_cumnet[strategy_name][:i])
    Max_Drawdown = (
        1 - performance_cumnet[strategy_name][i] / performance_cumnet[strategy_name][j]
    )

    # å¡ç›æ¯”ç‡
    Calmar = (Strategy_Annualized_Return_EAR) / Max_Drawdown

    # è¶…é¢æ”¶ç›Š
    Alpha_Final_Return = performance_cumnet[alpha_name].iloc[-1] - 1

    # è¶…é¢å¹´åŒ–æ”¶ç›Š
    Alpha_Annualized_Return_EAR = (1 + Alpha_Final_Return) ** (
        252 / len(performance_cumnet)
    ) - 1

    # è¶…é¢æ³¢åŠ¨ç‡
    Alpha_Volatility = performance_pct[alpha_name].std() * np.sqrt(252)

    # è¶…é¢å¤æ™®
    Alpha_Sharpe = (Alpha_Annualized_Return_EAR - rf) / Alpha_Volatility

    # è¶…é¢æœ€å¤§å›æ’¤
    i = np.argmax(
        (
            np.maximum.accumulate(performance_cumnet[alpha_name])
            - performance_cumnet[alpha_name]
        )
        / np.maximum.accumulate(performance_cumnet[alpha_name])
    )
    j = np.argmax(performance_cumnet[alpha_name][:i])
    Alpha_Max_Drawdown = (
        1 - performance_cumnet[alpha_name][i] / performance_cumnet[alpha_name][j]
    )

    # èƒœç‡
    performance_pct["win"] = performance_pct[alpha_name] > 0
    Win_Ratio = performance_pct["win"].value_counts().loc[True] / len(performance_pct)

    # ç›ˆäºæ¯”
    profit_lose = performance_pct.groupby("win")[alpha_name].mean()
    Profit_Lose_Ratio = abs(profit_lose[True] / profit_lose[False])

    result = {
        "ç­–ç•¥ç´¯è®¡æ”¶ç›Š": round(Strategy_Final_Return, 4),
        "ç­–ç•¥å¹´åŒ–æ”¶ç›Š": round(Strategy_Annualized_Return_EAR, 4),
        "ç­–ç•¥å¹´åŒ–æ”¶ç›Š(ç®—æœ¯)": round(Strategy_Annualized_Return_AM, 4),
        "åŸºå‡†ç´¯è®¡æ”¶ç›Š": round(Benchmark_Final_Return, 4),
        "åŸºå‡†å¹´åŒ–æ”¶ç›Š": round(Benchmark_Annualized_Return_EAR, 4),
        "é˜¿å°”æ³•": round(Alpha, 4),
        "è´å¡”": round(Beta, 4),
        "æ³¢åŠ¨ç‡": round(Strategy_Volatility, 4),
        "å¤æ™®æ¯”ç‡": round(Strategy_Sharpe, 4),
        "å¤æ™®æ¯”ç‡(ç®—æœ¯)": round(Strategy_Sharpe_AM, 4),
        "ä¸‹è¡Œæ³¢åŠ¨ç‡": round(Strategy_Down_Volatility, 4),
        "ç´¢æè¯ºæ¯”ç‡": round(Sortino, 4),
        "è·Ÿè¸ªè¯¯å·®": round(Tracking_Error, 4),
        "ä¿¡æ¯æ¯”ç‡": round(Information_Ratio, 4),
        "æœ€å¤§å›æ’¤": round(Max_Drawdown, 4),
        "å¡ç›æ¯”ç‡": round(Calmar, 4),
        "è¶…é¢ç´¯è®¡æ”¶ç›Š": round(Alpha_Final_Return, 4),
        "è¶…é¢å¹´åŒ–æ”¶ç›Š": round(Alpha_Annualized_Return_EAR, 4),
        "è¶…é¢æ³¢åŠ¨ç‡": round(Alpha_Volatility, 4),
        "è¶…é¢å¤æ™®": round(Alpha_Sharpe, 4),
        "è¶…é¢æœ€å¤§å›æ’¤": round(Alpha_Max_Drawdown, 4),
        "èƒœç‡": round(Win_Ratio, 4),
        "ç›ˆäºæ¯”": round(Profit_Lose_Ratio, 4),
    }

    performance_cumnet.plot(secondary_y="alpha", figsize=(10, 6))
    print(pd.DataFrame([result]).T)

    return performance_cumnet, result
